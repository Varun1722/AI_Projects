# -*- coding: utf-8 -*-
"""Fine-Tuning LLMs with Hugging Face

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RYuHD7jhEeORVB_hT7Wa6a2GbbMd7KN1

# Fine-Tuning LLMs with Hugging Face

This is a practical fine tuning of llama-2 (an open source LLM by Meta) on a large medical dataset. We want our model to learn about most of the medical terms out there and train it so that when any query is raised, our model can accurately reply to the query.

The Original dataset: [Wiki_medical_dataset](https://huggingface.co/datasets/gamino/wiki_medical_terms/viewer/default/train?p=2&row=217)

The Formatted dataset acc to llama: [medical_dataset_llama_format](https://huggingface.co/datasets/aboonaji/wiki_medical_terms_llam2_format) (This is used for training the model)

## Step 1: Installing and importing the libraries

We first install the important libraries needed for our project.
"""

!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7

!pip install huggingface_hub

import torch
from trl import SFTTrainer
from peft import LoraConfig
from datasets import load_dataset
from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline)

"""## Step 2: Loading the model

Here we load the model using a class of `transformer` library in `HuggingFace`. The parameters we use are first the path of the model we will be using i.e. Pre-trained llama-2 model and `load_in_4bit` precision so that our model size is reduced and so does our memory usage.

Now as `llama_2_model` is an object of `AutoModelForCausalLM` class so we can use one of its attributes, which is the `config.use_cache` attribute set it to False, and that means that we're not gonna store in the cache memory, the output of the previously computed layers, and that will of course allow to reduce the memory usage also speed up the training computations.

Now we set another attribute called `config.pretraining_tp` equal to 1  and what it will do is that it will deactivate the the more accurate computation of the linear layers, 'cause if we keep them activated, this will considerably slow down the linear layers computations.
"""

llama_2_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path = "aboonaji/llama2finetune-v3",
                                                     quantization_config = BitsAndBytesConfig(load_in_4bit=True ,
                                                                                              bnb_4bit_compute_dtype = getattr(torch,"float16"),
                                                                                              bnb_4bit_quant_type='nf4'))

llama_2_model.config.use_cache = False
llama_2_model.config.pretraining_tp = 1

"""## Step 3: Loading the tokenizer

Now that we have the model, we need to load as well a tokenizer that is compatible with this llama2 model while ensuring that the tokenizer uses the same special tokens as the model and the same padding.

A tokenizer converts your input into a format that can be processed by the model.

The second argument `trust_remote_code` is set to True, in order to say that we trust the source of where we are loading both the model and the tokenizer, meaning HuggingFace.

And after loading the tokenizer we do two more things:



1) **Padding**: We're gonna make sure that the pad token is the same as the end of sequence token so that all the sequences are of same length.

2)**Right Padding**
"""

llama_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = "aboonaji/llama2finetune-v3", trust_remote_code = True)
llama_tokenizer.pad_token = llama_tokenizer.eos_token
llama_tokenizer.padding_side = "right"

"""## Step 4: Setting the training arguments

`Why setting these training args separately? `

Because in the next step we're gonna create the supervised fine tuning trainer
as an instance of the SFT trainer class. And this SFT trainer class will take several arguments, one of which will be the training arguments and these training arguments will be created as an object of another class, which is called the training arguments class, and which allows to configure the training parameters of the future training that will happen once we retrain our llama tomorrow with the new data set containing the medical terms.
"""

training_args = TrainingArguments(output_dir="./results" , per_device_train_batch_size=4,max_steps=100)

"""## Step 5: Creating the Supervised Fine-Tuning trainer

There are two main techniques to develop your LLM with knowledge augmentation. The first one is `Supervised fine tuning`, which is a transfer learning technique in which the weights of a pre-trained model are trained on new data, which is here, the data containing all these advanced medical terms.

And the second technique is `RLHF`, reinforcement learning from human feedback, which is a much longer process and which requires of course human feedback, you know, through reinforcement learning.

The T4 GPU only has 12.7 gigabytes of RAM, and that's not enough to store the weight or pre-trained llama_2. And therefore we need to do what is called `**parameter efficient fine tuning**`,which is a technique that will reduce the amount of parameters that are gonna be fine tuned, as opposed to fully tuning all the parameters of the model
"""

sft_trainer = SFTTrainer(model = llama_2_model,
                         args = training_args,
                         train_dataset = load_dataset(path = "aboonaji/wiki_medical_terms_llam2_format",split = "train"),
                         tokenizer = llama_tokenizer,
                         peft_config = LoraConfig(task_type = 'CAUSAL_LM',r=64,lora_alpha = 16, lora_dropout=0.1),
                         dataset_text_field = "text")

"""## Step 6: Training the model"""

sft_trainer.train()

"""## Step 7: Chatting with the model"""

user_prompt = "Discuss about the disease Lentigo in points"
text_generation_pipeline = pipeline(task = 'text-generation',model=llama_2_model, tokenizer=llama_tokenizer,max_length = 400)
prompt_answer = text_generation_pipeline(f"<s>[INST] {user_prompt} [\INST]")
print(prompt_answer[0]["generated_text"])